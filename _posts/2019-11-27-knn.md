---
excerpt: "(1) knn"
header:
  overlay_image: /assets/images/2computers.jpg
  overlay_filter: 0.5 # same as adding an opacity of 0.5 to a black background
  caption: "Photo credit: [**Unsplash**]"
  actions:
    - label: "Reference"
      url: "https://docs.python.org/3/"
title: "[Python] knn"
date: 2019-11-27 00:00:00 -0400
categories: knn
tags: knn
gallery1:
  - url: /assets/images/dummydata.JPG
    image_path: assets/images/dummydata.JPG
    alt: "placeholder image"
gallery2:
  - url: /assets/images/train_test.JPG
    image_path: assets/images/train_test.JPG
    alt: "placeholder image"
gallery3:
  - url: /assets/images/roc_1.JPG
    image_path: assets/images/roc_1.JPG
    alt: "placeholder image"    
---

---
**본 과정은 제가 파이썬 수업을 들으면서 배운 내용을 복습하는 과정에서 적어본 것입니다.<br> 틀린 부분이 있다면 댓글에 남겨주시면 고치도록 하겠습니다.<br> 확실하지 않은 내용은 '(?)'을 함께 적었으니 그 내용을 아신다면 댓글에 남겨주시면 감사하겠습니다.** 
{: .notice--warning}
--- 

# KNN 

KNN은 이상치에 크게 영향을 받는다. 따라서 Normalization을 쓴다. 

min-max는 0과 1 사이로 값을 데이터 값을 줄이기 때문에 크기 차이가 없어진다. 즉, 중요도 차이가 없어진다는 것이다. 
따라서 Standard Scaler를 더 많이 쓴다. 

사진 분류도 VGG16부터는 standard scaler를 함께 쓴다. 

## sklearn.cluster - KMeans

- 예제 데이터 로드

```python
from sklearn.datasets import load_iris

data = load_iris()
```

- DBSCAN 

먼저 몇개의 군집으로 나눌지 k를 찾는 용으로 사용할 수 있다. 가장 최고의 클러스터 개수를 찾아준다. 

```python
from sklearn.cluster import KMeans, DBSCAN
dbs = DBSCAN(min_samples=10)
# DBSCAN 해서 k를 찾으면 그 떄 knn를 한다. 

dbs.fit_predict(data.data) # 대충 3개의 군집으로 나눠지는 것을 확인할 수 있다.
'''
array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,
        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,  1,
        1,  1,  1,  1,  1,  1, -1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,
       -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
        1,  1, -1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1, -1,  1, -1,  1,
       -1,  1,  1, -1, -1, -1, -1, -1,  1,  1,  1, -1, -1,  1,  1, -1, -1,
       -1,  1, -1, -1,  1,  1, -1,  1,  1,  1, -1, -1, -1,  1,  1, -1, -1,
       -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1],
      dtype=int64)
'''
```

```python
kme = KMeans(3)
kme.fit_predict(data.data)
'''
array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0,
       0, 0, 0, 2, 2, 0, 0, 0, 0, 2, 0, 2, 0, 2, 0, 0, 2, 2, 0, 0, 0, 0,
       0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 2])
'''
```

## opencv2 knn

cv는 기본적으로 float32를 사용한다. 또한 한글도 안된다. 

opencv는 시스템 만드는데 효과적이기 때문에 시스템 만들때 쓴다. 

```python
# 더미 데이터셋 생성
trainData = np.random.randint(0, 100, (25,2)).astype(np.float32)
responses = np.random.randint(0, 2, (25,1)).astype(np.float32)

from sklearn.datasets import make_classification

red = trainData[responses.ravel()==0]
plt.scatter(red[:,0], red[:,1], 80, "r", "^")

blue = trainData[responses.ravel()==1]
plt.scatter(blue[:,0], blue[:,1], 80, "b", "s")

newcomer = np.random.randint(0,100,(1,2)).astype(np.float32)
plt.scatter(newcomer[:,0], newcomer[:,1], 80, "g", ".")

plt.show()
```

{% include gallery id="gallery1" caption="dummydata" %}



```python
knn = cv2.ml.KNearest_create()
# 학습
knn.train(trainData, cv2.ml.ROW_SAMPLE, responses)
# True
# 찾기
knn.findNearest(newcomer, k=5) # predict
'''

(0.0,
 array([[0.]], dtype=float32),
 array([[0., 0., 0., 1., 0.]], dtype=float32),
 array([[146., 146., 205., 290., 405.]], dtype=float32))
'''
ret, results, neighbours, dist = knn.findNearest(newcomer, k=5)
```


### iris 데이터로 실습 

```python
import imutils
from sklearn.datasets import load_iris
iris = load_iris()

knn = cv2.ml.KNearest_create()
iris.data.dtype, iris.target.dtype
# (dtype('float64'), dtype('int32'))

trainData = iris.data.astype(np.float32)
responses = iris.target.astype(np.float32)

knn.train(trainData, cv2.ml.ROW_SAMPLE, responses)
# True

ret, results, neighbours, dist = knn.findNearest(trainData, k=5)

knn.findNearest(np.float32([[3,3,3,3]]), 3) # type를 맞춰야한다. 
'''
(1.0,
 array([[1.]], dtype=float32),
 array([[2., 1., 1.]], dtype=float32),
 array([[7.8      , 8.06     , 8.2699995]], dtype=float32))
'''

# Accuracy
matches = results
correct = np.count_nonzero(matches)
accuracy = correct*100.0/results.size
accuracy 
# 66.66666666666667

# knn.calcError() : scikit에서의 score와 같다. 
```




{% capture notice-2 %}

{% endcapture %}

<div class="notice">{{ notice-2 | markdownify }}</div>






{% capture notice-2 %}

{% endcapture %}

<div class="notice">{{ notice-2 | markdownify }}</div>




---
**무단 배포 금지** 
{: .notice--danger}
