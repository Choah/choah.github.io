---
excerpt: "(1) knn (2) kmeans (3) hierarchical (4) dbscan"
header:
  overlay_image: /assets/images/2computers.jpg
  overlay_filter: 0.5 # same as adding an opacity of 0.5 to a black background
  caption: "Photo credit: [**Unsplash**]"
  actions:
    - label: "Reference"
      url: "https://docs.python.org/3/"
title: "[Python] cluster"
date: 2019-11-27 00:00:00 -0400
categories: cluster
tags: knn kmeans hierarchical dbscan
gallery1:
  - url: /assets/images/dummydata.JPG
    image_path: assets/images/dummydata.JPG
    alt: "placeholder image"
gallery2:
  - url: /assets/images/train_test.JPG
    image_path: assets/images/train_test.JPG
    alt: "placeholder image"
gallery3:
  - url: /assets/images/roc_1.JPG
    image_path: assets/images/roc_1.JPG
    alt: "placeholder image"    
---

---
**본 과정은 제가 파이썬 수업을 들으면서 배운 내용을 복습하는 과정에서 적어본 것입니다.<br> 틀린 부분이 있다면 댓글에 남겨주시면 고치도록 하겠습니다.<br> 확실하지 않은 내용은 '(?)'을 함께 적었으니 그 내용을 아신다면 댓글에 남겨주시면 감사하겠습니다.** 
{: .notice--warning}
--- 

# KNN 

KNN은 이상치에 크게 영향을 받는다. 따라서 Normalization을 쓴다. 

min-max는 0과 1 사이로 값을 데이터 값을 줄이기 때문에 크기 차이가 없어진다. 즉, 중요도 차이가 없어진다는 것이다. 
따라서 Standard Scaler를 더 많이 쓴다. 

사진 분류도 VGG16부터는 standard scaler를 함께 쓴다. 

## sklearn.cluster - DBSCAN, KMeans, AgglomerativeClustering 

- 예제 데이터 로드

```python
from sklearn.datasets import load_iris

data = load_iris()
```

- DBSCAN 

먼저 몇개의 군집으로 나눌지 k를 찾는 용으로 사용할 수 있다. 가장 최고의 클러스터 개수를 찾아준다. 

```python
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
dbs = DBSCAN(min_samples=10)
# DBSCAN 해서 k를 찾으면 그 떄 knn를 한다. 

dbs.fit_predict(data.data) # 대충 3개의 군집으로 나눠지는 것을 확인할 수 있다.
'''
array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,
        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,  1,
        1,  1,  1,  1,  1,  1, -1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,
       -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
        1,  1, -1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1, -1,  1, -1,  1,
       -1,  1,  1, -1, -1, -1, -1, -1,  1,  1,  1, -1, -1,  1,  1, -1, -1,
       -1,  1, -1, -1,  1,  1, -1,  1,  1,  1, -1, -1, -1,  1,  1, -1, -1,
       -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1],
      dtype=int64)
'''
```

- KMeans

Unsupervised data를 분류할 수 있습니다. 
하지만 단점으로 초기값을 랜덤하게 찍기 때문에, 초기값에 따라 결과가 다릅니다. 따라서 크게 활용되지 않습니다.

```python
kme = KMeans(3)
kme.fit_predict(data.data)
'''
array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0,
       0, 0, 0, 2, 2, 0, 0, 0, 0, 2, 0, 2, 0, 2, 0, 0, 2, 2, 0, 0, 0, 0,
       0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 2])
'''
```

- AgglomerativeClustering 

hierarchical cluster 입니다. 

```python
from sklearn.cluster import AgglomerativeClustering
agg = AgglomerativeClustering()
agg.fit(iris.iloc[:,:-1])
'''
AgglomerativeClustering(affinity='euclidean', compute_full_tree='auto',
                        connectivity=None, distance_threshold=None,
                        linkage='ward', memory=None, n_clusters=3,
                        pooling_func='deprecated')
'''
vars(agg)
'''
{'n_clusters': 3,
 'distance_threshold': None,
 'memory': None,
 'connectivity': None,
 'compute_full_tree': 'auto',
 'linkage': 'ward',
 'affinity': 'euclidean',
 'pooling_func': 'deprecated',
 'children_': array([[101, 142],
        [  7,  39],
        [  0,  17],
        [  9,  34],
        [128, 132],
        [ 10,  48],
        [  4,  37],
        [ 19,  21],
        [ 29,  30],
        [ 57,  93],
        [ 80,  81],
        [116, 137],
        [  8,  38],
        [  3,  47],
        [ 27,  28],
        [ 82,  92],
        [ 95,  96],
        [127, 138],
        [  1,  45],
        [ 63,  91],
        [ 65,  75],
        [ 40, 152],
        [123, 126],
        [ 49, 151],
        [112, 139],
        [ 94,  99],
        [ 12, 168],
        [ 88, 166],
        [ 66,  84],
        [ 23,  26],
        [ 53,  89],
        [ 74,  97],
        [ 25, 153],
        [ 46, 157],
        [  2, 163],
        [110, 147],
        [120, 143],
        [136, 148],
        [ 78, 169],
        [ 69, 160],
        [ 54,  58],
        [140, 144],
        [141, 145],
        [ 43, 179],
        [ 68,  87],
        [ 50,  52],
        [ 51,  56],
        [107, 130],
        [105, 122],
        [103, 161],
        [164, 171],
        [ 20,  31],
        [ 11, 158],
        [ 67, 165],
        [ 70, 167],
        [ 42, 162],
        [113, 150],
        [  6, 184],
        [173, 200],
        [ 55,  90],
        [176, 182],
        [ 86, 195],
        [124, 186],
        [ 83, 133],
        [  5,  18],
        [ 13, 205],
        [175, 177],
        [ 32,  33],
        [125, 129],
        [104, 154],
        [ 73, 188],
        [149, 204],
        [146, 172],
        [121, 206],
        [ 36, 155],
        [ 76, 190],
        [115, 187],
        [ 61,  71],
        [156, 208],
        [ 72, 213],
        [117, 131],
        [191, 212],
        [ 24, 202],
        [ 98, 159],
        [ 16, 224],
        [ 35, 210],
        [ 64,  79],
        [ 85, 196],
        [ 77, 185],
        [ 44, 183],
        [111, 199],
        [180, 189],
        [102, 218],
        [174, 192],
        [181, 227],
        [170, 225],
        [118, 198],
        [ 14,  15],
        [178, 209],
        [222, 229],
        [201, 234],
        [114, 223],
        [ 60, 233],
        [217, 247],
        [ 59, 241],
        [207, 232],
        [197, 242],
        [ 62, 203],
        [214, 250],
        [119, 194],
        [100, 226],
        [108, 219],
        [216, 248],
        [211, 245],
        [240, 261],
        [193, 239],
        [109, 135],
        [235, 255],
        [238, 243],
        [236, 254],
        [ 22, 215],
        [220, 244],
        [228, 265],
        [257, 269],
        [134, 249],
        [221, 237],
        [231, 260],
        [ 41, 270],
        [230, 266],
        [106, 262],
        [253, 258],
        [259, 274],
        [267, 277],
        [264, 268],
        [271, 275],
        [246, 278],
        [251, 281],
        [276, 283],
        [256, 285],
        [273, 279],
        [272, 280],
        [263, 284],
        [252, 289],
        [286, 291],
        [282, 290],
        [287, 288],
        [292, 293],
        [295, 296],
        [294, 297]], dtype=int64),
 'n_connected_components_': 1,
 'n_leaves_': 150,
 'n_clusters_': 3,
 'labels_': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2,
        2, 2, 2, 0, 0, 2, 2, 2, 2, 0, 2, 0, 2, 0, 2, 2, 0, 0, 2, 2, 2, 2,
        2, 0, 0, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 0], dtype=int64)}
'''
```


## opencv2 knn

cv는 기본적으로 float32를 사용한다. 또한 한글도 안된다. 

opencv는 시스템 만드는데 효과적이기 때문에 시스템 만들때 쓴다. 

```python
# 더미 데이터셋 생성
trainData = np.random.randint(0, 100, (25,2)).astype(np.float32)
responses = np.random.randint(0, 2, (25,1)).astype(np.float32)

from sklearn.datasets import make_classification

red = trainData[responses.ravel()==0]
plt.scatter(red[:,0], red[:,1], 80, "r", "^")

blue = trainData[responses.ravel()==1]
plt.scatter(blue[:,0], blue[:,1], 80, "b", "s")

newcomer = np.random.randint(0,100,(1,2)).astype(np.float32)
plt.scatter(newcomer[:,0], newcomer[:,1], 80, "g", ".")

plt.show()
```

{% include gallery id="gallery1" caption="dummydata" %}



```python
knn = cv2.ml.KNearest_create()
# 학습
knn.train(trainData, cv2.ml.ROW_SAMPLE, responses)
# True
# 찾기
knn.findNearest(newcomer, k=5) # predict
'''

(0.0,
 array([[0.]], dtype=float32),
 array([[0., 0., 0., 1., 0.]], dtype=float32),
 array([[146., 146., 205., 290., 405.]], dtype=float32))
'''
ret, results, neighbours, dist = knn.findNearest(newcomer, k=5)
```


### iris 데이터로 실습 

```python
import imutils
from sklearn.datasets import load_iris
iris = load_iris()

knn = cv2.ml.KNearest_create()
iris.data.dtype, iris.target.dtype
# (dtype('float64'), dtype('int32'))

trainData = iris.data.astype(np.float32)
responses = iris.target.astype(np.float32)

knn.train(trainData, cv2.ml.ROW_SAMPLE, responses)
# True

ret, results, neighbours, dist = knn.findNearest(trainData, k=5)

knn.findNearest(np.float32([[3,3,3,3]]), 3) # type를 맞춰야한다. 
'''
(1.0,
 array([[1.]], dtype=float32),
 array([[2., 1., 1.]], dtype=float32),
 array([[7.8      , 8.06     , 8.2699995]], dtype=float32))
'''

# Accuracy
matches = results
correct = np.count_nonzero(matches)
accuracy = correct*100.0/results.size
accuracy 
# 66.66666666666667

# knn.calcError() : scikit에서의 score와 같다. 
```





{% capture notice-2 %}

{% endcapture %}

<div class="notice">{{ notice-2 | markdownify }}</div>






{% capture notice-2 %}

{% endcapture %}

<div class="notice">{{ notice-2 | markdownify }}</div>




---
**무단 배포 금지** 
{: .notice--danger}
